{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11755f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.io import wavfile\n",
    "from textgrid import TextGrid as TG\n",
    "from textgrid import Interval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fec5f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (3336785297.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_526/3336785297.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break # Old, simplified vesion. Do not use anymore.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break # Old, simplified vesion. Do not use anymore.\n",
    "\n",
    "root_folder = '../../allwavs/allvowl'\n",
    "\n",
    "# Read all input files\n",
    "all_files = os.listdir(root_folder)\n",
    "all_lower_files = [file.lower() for file in all_files]\n",
    "corpus = defaultdict(list)\n",
    "final_corpus = {}\n",
    "\n",
    "variant = True\n",
    "\n",
    "# Keep the name of the speaker (without additional information) as the key of each file\n",
    "for file in all_files:\n",
    "    file_l = file.lower().replace('_1', '').replace('cut', '').replace('-corrected', '')\n",
    "    if file_l.endswith('wav'):\n",
    "        corpus[file_l[:-4]].append(file)\n",
    "    if file_l.endswith('textgrid'):\n",
    "        corpus[file_l[:-9]].append(file)\n",
    "\n",
    "for k, v in corpus.items():\n",
    "    # Only keep speakers for which we have both audio and transcriptions\n",
    "    if not all(not x.lower().endswith('textgrid') for x in v) or all(not x.lower().endswith('wav') for x in v):\n",
    "        wavs = [file for file in v if file.lower().endswith('wav')]\n",
    "        \n",
    "        # If multiple files are available, keep only the ones with 'cut' in the name\n",
    "        if len(wavs) > 1:\n",
    "            wavs = [x for x in wavs if 'cut' in x]\n",
    "        wav = wavs[0]\n",
    "        \n",
    "        # If multiple transcription files, choose which one to keep\n",
    "        textgrids = [file for file in v if file.lower().endswith('textgrid')]\n",
    "        if len(textgrids) > 1:\n",
    "            if variant:\n",
    "                textgrids = [x for x in textgrids if '_1' in x]\n",
    "            else:\n",
    "                textgrids = [x for x in textgrids if 'mono.' in x]\n",
    "            # Exception: One speaker has only one type of transcription\n",
    "            if k == 'om1_mono':\n",
    "                textgrids = ['om1_mono.TextGrid']\n",
    "        textgrid = textgrids[0]\n",
    "        final_corpus[k] = [wav, textgrid]\n",
    "    else:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '../../allwavs/allvowl/corrected'\n",
    "\n",
    "all_files = os.listdir(root_folder)\n",
    "final_corpus = {}\n",
    "\n",
    "variant = True\n",
    "\n",
    "# Generate a dictionary of speaker - (audio file, transcription file) key-values\n",
    "for file in all_files:\n",
    "    if file.endswith('wav'):\n",
    "        file_l = file.replace('cut', '')\n",
    "        final_corpus[file_l.replace('.wav', '')] = (file, file.replace('wav', 'textgrid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf828162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████████████████████████▊                        | 52/67 [01:39<00:30,  2.04s/it]/tmp/ipykernel_576/719033392.py:76: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, wave_data = wavfile.read(in_wav)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [02:10<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "marks = set()\n",
    "\n",
    "# Set of all available vowels\n",
    "vowels = {'2', '9', '@', 'E', 'E/', 'O', 'O/', 'U~/', 'a', 'a~', 'e', 'i', 'o', 'o~', 'u', 'y'}\n",
    "\n",
    "# Maps a speaker to their gender (f or m) and their pronunciation of /E/ in open syllables (/e/ or /E/)\n",
    "info_dict = {\n",
    "    'ac1': 'fe',\n",
    "    'ac2': 'fe',\n",
    "    'agnes1': 'fE',\n",
    "    'agnes2': 'fE',\n",
    "    'al1': 'fe',\n",
    "    'al2': 'fe',\n",
    "    'al': 'fe',\n",
    "    'ang1': 'fe',\n",
    "    'ang2': 'fe',\n",
    "    'be1': 'mE',\n",
    "    'be2': 'mE',\n",
    "    'cg1': 'fE',\n",
    "    'cg2': 'fE',\n",
    "    'clement': 'me',\n",
    "    'df': 'mE',\n",
    "    'dj1': 'mE',\n",
    "    'dj2': 'me',\n",
    "    'dorian': 'mE',\n",
    "    'em1': 'mE',\n",
    "    'em2': 'mE',\n",
    "    'ev1': 'mE',\n",
    "    'ev2': 'mE',\n",
    "    'hzc1': 'fe',\n",
    "    'hzc2': 'fe',\n",
    "    'ib1': 'fe',\n",
    "    'ib2': 'fe',\n",
    "    'isabel22': 'fE',\n",
    "    'isabel2': 'fE',\n",
    "    'jb1': 'mE',\n",
    "    'jb2': 'mE',\n",
    "    'kd': 'me',\n",
    "    'ken1': 'me',\n",
    "    'laure': 'fe',\n",
    "    'lb1': 'fE',\n",
    "    'lb2': 'fE',\n",
    "    'lp1': 'me',\n",
    "    'lp2': 'me',\n",
    "    'lucas': 'me',\n",
    "    'mat1': 'mE',\n",
    "    'mathieu2': 'mE',\n",
    "    'melanie1': 'fE',\n",
    "    'melanie2': 'fE',\n",
    "    'mob': 'me',\n",
    "    'om1': 'fE',\n",
    "    'om2': 'fE',\n",
    "    'or1': 'fE',\n",
    "    'or2': 'fE',\n",
    "    'oriane-re2': 'fE',\n",
    "    'oriane1': 'fE',\n",
    "    'phil1': 'mE',\n",
    "    'phil2': 'mE',\n",
    "    'simon1': 'mE',\n",
    "    'simon2': 'mE',\n",
    "    'unk1': 'fe',\n",
    "    'unk2': 'fe',\n",
    "    'vb1': 'fE',\n",
    "    'vb2': 'fE',\n",
    "    'vc1': 'mE',\n",
    "    'vc2': 'mE',\n",
    "    'vg': 'mE',\n",
    "    'vibl1': 'fE',\n",
    "    'vibl2': 'fE',\n",
    "    'ya1': 'mE',\n",
    "    'ya2': 'mE',\n",
    "    'yf1': 'me',\n",
    "    'yf2': 'me',\n",
    "    'yl': 'mE'\n",
    "}\n",
    "\n",
    "def trim_wav(in_wav, out_wav, start, end):\n",
    "    \"\"\"\n",
    "    Crops the sound contained in an input wave file at a given start and end (in seconds),\n",
    "    and saves the result into an output wave file\n",
    "    \"\"\"\n",
    "    sample_rate, wave_data = wavfile.read(in_wav)\n",
    "    start_sample = int(start * sample_rate)\n",
    "    end_sample = int(end * sample_rate)\n",
    "    wavfile.write(out_wav, sample_rate, wave_data[start_sample:end_sample])\n",
    "\n",
    "def __hash__(self):\n",
    "    # Needed to have Interval's in sets\n",
    "    return hash((self.minTime, self.maxTime, self.mark))\n",
    "\n",
    "Interval.__hash__ = __hash__\n",
    "\n",
    "trimmed = False # Choose cropped (vowel only) or uncropped (full recording) sound files\n",
    "\n",
    "for k, (wav, textgrid) in tqdm(final_corpus.items()):\n",
    "    # Loop other all speakers\n",
    "    \n",
    "    # Try to read transcription files\n",
    "    try:\n",
    "        tg = TG.fromFile(root_folder + '/' + textgrid)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    \n",
    "    prev_p = '' # The phoneme in the previous recording.\n",
    "    # The order is always 'l', 'm', 'p', 's', 't', 't1' (/t/ with falling intonation)\n",
    "    \n",
    "    two_mode = False # Are we currently in a series with nasal vowels? (written as '2' in output files)\n",
    "    out_phones = defaultdict(list) # Maps a word-level annotation to the list of corresponding phoneme-level annotations\n",
    "    gender, E_pron = info_dict[k.replace('_mono', '')]\n",
    "    if len(tg) == 2: # Make sure that we have two transcription tiers: phoneme-level and word-level\n",
    "        phontier, wordtier = tg\n",
    "        for phone in phontier: # For each non-silent phoneme-level annotation\n",
    "            if phone.mark != '#':\n",
    "                best_overlap_duration = 0 # Try to find the corresponding word-level annotation...\n",
    "                best_word = None\n",
    "                for word in wordtier: # ...by computing the overlap percentage with all word-level annotations\n",
    "                    overlap_duration = max(0, min(phone.maxTime, word.maxTime) - max(phone.minTime, word.minTime))\n",
    "                    if overlap_duration > best_overlap_duration:\n",
    "                        best_overlap_duration = overlap_duration\n",
    "                        best_word = word\n",
    "                out_phones[best_word].append(phone)\n",
    "        for word, phones in out_phones.items(): # For each (word, phonemes) mapping, generate the output filename\n",
    "            word2 = word.mark # Read the word transcription\n",
    "            phonemes = '_'.join([phone.mark for phone in phones])\n",
    "            if phonemes == prev_p: # Only happens if we are on a falling-intonation /t/ (t1)\n",
    "                phonemes = phonemes[0] + '1' + phonemes[1:] # Encode the phoneme name as such\n",
    "                word2 += '1' # and the word\n",
    "                if word.mark == 'tant': # If we are on the first word of the nasal vowel series\n",
    "                    two_mode = True\n",
    "            elif two_mode:\n",
    "                phonemes = phonemes[0] + '2' + phonemes[1:] # If on a nasal vowel, encode the phoneme name as such\n",
    "                word2 =  word2.replace('1', '') + '2'\n",
    "                \n",
    "            min_d = word.minTime\n",
    "            max_d = word.maxTime\n",
    "\n",
    "            if phones[1:]: # The phonemes are encoded into `phonemes`, only keep the vowel name for now (second vowel)\n",
    "                phones = phones[1:]\n",
    "            elif phones[0].mark not in vowels:\n",
    "                continue\n",
    "            \n",
    "            phone = phones[0]\n",
    "            saved_phone_mark = phone.mark # Save the vowel name\n",
    "            \n",
    "            # Exceptions #1 (people proouncing /e/ as /E/)\n",
    "            if k in ['cg1', 'cg2', 'ib1', 'ib2', 'mathieu2', 'melanie1', 'melanie2', 'vb1']:\n",
    "                if any(x in word.mark for x in ['pé', 'les', 'mes', 'ses', 'tes']):\n",
    "                    phone.mark = 'E'\n",
    "                    # Exceptions #2 (people proouncing /e/ as /E/ but only in some words)\n",
    "                    if 'pé' in word.mark and ('cg' in k or 'melanie' in k or 'vb1' in k) or \\\n",
    "                    ('ses' in word.mark or 'tes' in word.mark) and 'mathieu' in k:\n",
    "                        phone.mark = 'e'\n",
    "            elif any(x in word.mark for x in ['pé', 'les', 'mes', 'ses', 'tes']): # Standard pronunciation\n",
    "                phone.mark = 'e'\n",
    "            if any(x in word.mark for x in ['paix', 'lait', 'mais', 'sait', 'taie']): # Hard-coded pronunciation\n",
    "                phone.mark = E_pron\n",
    "            if word.mark == 'sœur': # Astali mis-annotated /s9R/ as /syR/\n",
    "                phonemes = phonemes.replace('y', '9')\n",
    "                phone.mark = '9'\n",
    "            \n",
    "            # Get rid of schwa and / in initial annotations (old annotations only)\n",
    "            phonemes = phonemes.replace(saved_phone_mark, phone.mark).replace('/', '').replace('@', '2')\n",
    "            phone.mark = phone.mark.replace('/', '').replace('@', '2')\n",
    "            add_path = 'trimmed/' if trimmed else ''\n",
    "            \n",
    "            # Get position of vowel in recording...\n",
    "            phon_start = phone.minTime - min_d\n",
    "            phon_end = phone.maxTime - min_d\n",
    "            \n",
    "            # ...as a percentage of the length of the file\n",
    "            if not trimmed:\n",
    "                phon_start /= (max_d - min_d)\n",
    "                phon_end /= (max_d - min_d)\n",
    "            # Save to file\n",
    "            trim_wav(f'{root_folder}/{wav}',\n",
    "                     f'{root_folder}/extracted/{add_path}{phone.mark}__{gender}__{phon_start:.5f}__{phon_end:.5f}__{phonemes}__{word2}__{wav}',\n",
    "                     phone.minTime if trimmed else min_d,\n",
    "                     phone.maxTime if trimmed else max_d)\n",
    "            prev = phones\n",
    "            prev_p = phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7904613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lambda Notebook (Python 3)",
   "language": "python",
   "name": "lambda-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
