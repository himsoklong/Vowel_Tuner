{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1a775d",
   "metadata": {},
   "source": [
    "## Run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159c05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import ipynb.fs\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wave\n",
    "\n",
    "from .defs.extract_formant import extract_formant\n",
    "from joblib import load\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_leading_silence\n",
    "from scipy.io import wavfile\n",
    "from skimage.transform import resize\n",
    "\n",
    "# The Regression model's definition\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        self.cnn_layer1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=3, padding='valid'), nn.ReLU(), nn.Dropout(0.1), nn.BatchNorm2d(16), nn.MaxPool2d(kernel_size=2))\n",
    "        self.cnn_layer2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, padding='valid'), nn.ReLU(), nn.Dropout(0.2), nn.BatchNorm2d(32), nn.MaxPool2d(kernel_size=2))\n",
    "        self.linear_layer1 = nn.Linear(32*30*6 + 8, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.activ1 = nn.ReLU()\n",
    "        self.linear_layer_p = nn.Linear(64, 32)\n",
    "        self.dropout_p = nn.Dropout(0.5)\n",
    "        self.activ_p = nn.ReLU()\n",
    "        self.linear_layer2 = nn.Linear(32, 2)\n",
    "        self.activ2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, images, features):\n",
    "        cnn2 = self.cnn_layer2(self.cnn_layer1(images.unsqueeze(1)))\n",
    "        cnn_vec = cnn2.reshape(cnn2.shape[0], -1)\n",
    "        out = self.dropout1(self.activ1(self.linear_layer1(torch.cat((cnn_vec, features), dim=1))))\n",
    "        return self.activ2(self.linear_layer2(self.activ_p(self.dropout_p(self.linear_layer_p(out)))))\n",
    "\n",
    "# Function for rescaling the melspectrogram\n",
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "# Function for generating the melspectrogram\n",
    "def spectrogram(y, sr):\n",
    "    # use log-melspectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=512, hop_length=512)\n",
    "    mels = np.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "    img = scale_minmax(mels, 0, 255).astype(np.uint8)\n",
    "    img = np.flip(img, axis=0) # put low frequencies at the bottom in image\n",
    "    img = 255-img\n",
    "    return img\n",
    "    \n",
    "clf = load('../models/rule_based.joblib') # The classifier\n",
    "scaler = load('../models/scaler.joblib') # The scaler, transforms formants so that they have a mean of 0 and a variance of 1\n",
    "regressor = torch.load('../models/neural_regressor.pt') # The vowel detection model\n",
    "\n",
    "trim_leading_silence: AudioSegment = lambda x: x[detect_leading_silence(x) :]\n",
    "trim_trailing_silence: AudioSegment = lambda x: trim_leading_silence(x.reverse()).reverse()\n",
    "strip_silence: AudioSegment = lambda x: trim_trailing_silence(trim_leading_silence(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2f7e9",
   "metadata": {},
   "source": [
    "## Run once per file (config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883d810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../../allwavs/allvowl/corrected/extracted/o__f__0.63636__1.00000__m_o__mot__hzc1_mono.wav' # Put your file here\n",
    "target_vowel = 'o' # Possible values: 'a', 'e', 'E', 'i', 'o', 'O', 'u', 'y', '2', '9'\n",
    "speaker_gender = 'f' # Possible values: 'f' or 'm'\n",
    "previous_phoneme = 'm' # Possible values: 'l', 'm', 'p', 's', 't' or 't1' (last one shouldn't be used)\n",
    "word_ends_with_r = False # True if there is an /R/ after the vowel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59385651",
   "metadata": {},
   "source": [
    "## Run once per file (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf01e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vowel  Confidence\n",
      "-------------------------\n",
      "2      0.004 \n",
      "9      0.000 \n",
      "a      0.000 \n",
      "e      0.008 \n",
      "E      0.000 \n",
      "i      0.012 =\n",
      "O      0.000 \n",
      "o      0.572 =========================================================\n",
      "u      0.390 =======================================\n",
      "y      0.014 =\n",
      "Prediction: /o/, confidence: 0.572\n"
     ]
    }
   ],
   "source": [
    "idx2key = ['2', '9', 'a', 'a~', 'e', 'E', 'i', 'O', 'o', 'o~', 'u', 'U~+', 'y'] # All possible vowels\n",
    "valid = [0, 1, 2, 4, 5, 6, 7, 8, 10, 12] # Vowels we consider here (depends on the classifier)\n",
    "all_phonemes = ['l', 'm', 'p', 's', 't', 't1'] # Phonemes that can be before the vowel\n",
    "\n",
    "tmp_wav = 'tmp_process.wav'\n",
    "tmp_wav_2 = 'tmp_process_trimmed.wav'\n",
    "max_w = 31 # Image width to resize to\n",
    "\n",
    "# Remove leading and trailing silences\n",
    "sound = AudioSegment.from_file(input_file)\n",
    "trimmed = strip_silence(sound)\n",
    "trimmed.export(tmp_wav, format='wav', bitrate='768k')\n",
    "    \n",
    "# Generate log-melspectrogram\n",
    "y, sr = librosa.load(tmp_wav)\n",
    "melspec = spectrogram(y=y, sr=sr)\n",
    "\n",
    "# Feed log-melspectrogram to regression model to predict start and and of the vowel\n",
    "melspec = resize(melspec, (melspec.shape[0], max_w), anti_aliasing=False)\n",
    "input_tensor = torch.tensor(melspec).float().cuda()\n",
    "input_features = torch.tensor([speaker_gender == 'f', not word_ends_with_r, *[previous_phoneme == x for x in all_phonemes]]).cuda()\n",
    "pred = regressor(input_tensor.unsqueeze(0), input_features.unsqueeze(0))[0]\n",
    "vowel_start = pred[0].item()\n",
    "vowel_end = pred[1].item()\n",
    "if vowel_start >= vowel_end:\n",
    "    print('The model predicted that the vowel has negative duration!')\n",
    "    raise ValueError()\n",
    "\n",
    "# Trim file at start and end to only have the vowel\n",
    "sample_rate, wave_data = wavfile.read(tmp_wav)\n",
    "duration = len(wave_data) / sample_rate\n",
    "start_sample = int(duration * vowel_start * sample_rate)\n",
    "end_sample = int(duration * vowel_end * sample_rate)\n",
    "wavfile.write(tmp_wav_2, sample_rate, wave_data[start_sample:end_sample])\n",
    "duration = len(wave_data[start_sample:end_sample]) / sample_rate\n",
    "\n",
    "# Extract formants\n",
    "try:\n",
    "    formants = extract_formant(tmp_wav_2, f0min=math.ceil(3/duration + 0.000001), n_formants=4)\n",
    "except ZeroDivisionError:\n",
    "    print('The file is too short to analyze!')\n",
    "    raise\n",
    "\n",
    "# Add additional features (gender, previous phoneme)\n",
    "input_features = torch.cat([input_features[0:1], input_features[2:]]).cpu()\n",
    "features = torch.cat((torch.tensor(formants), input_features)).numpy()\n",
    "\n",
    "# Rescale formants\n",
    "features[:4] = scaler.transform(np.array(features[:4]).reshape(1, -1))[0]\n",
    "\n",
    "# Prediction with probabilities\n",
    "pred = clf.predict_proba([features]) # Probabilities\n",
    "final_vowel = np.argmax(pred)\n",
    "final_confidence = pred[0][final_vowel] # Best score\n",
    "final_vowel = idx2key[valid[final_vowel]] # Actual prediction\n",
    "\n",
    "print('Vowel ', 'Confidence')\n",
    "print('-'*25)\n",
    "for i in range(len(valid)):\n",
    "    vowel = idx2key[valid[i]]\n",
    "    print(f'{vowel:<6} {pred[0][i]:.3f}', '='*int(pred[0][i]*100))\n",
    "\n",
    "print(f'Prediction: /{final_vowel}/, confidence: {final_confidence:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f4475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lambda Notebook (Python 3)",
   "language": "python",
   "name": "lambda-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
