{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1a775d",
   "metadata": {},
   "source": [
    "## Run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "159c05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import ipynb.fs\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wave\n",
    "\n",
    "from joblib import load\n",
    "from parselmouth import praat\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_leading_silence\n",
    "from scipy.io import wavfile\n",
    "from skimage.transform import resize\n",
    "\n",
    "# The Regression model's definition\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        self.cnn_layer1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=3, padding='valid'), nn.ReLU(), nn.Dropout(0.1), nn.BatchNorm2d(16), nn.MaxPool2d(kernel_size=2))\n",
    "        self.cnn_layer2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, padding='valid'), nn.ReLU(), nn.Dropout(0.2), nn.BatchNorm2d(32), nn.MaxPool2d(kernel_size=2))\n",
    "        self.linear_layer1 = nn.Linear(32*30*6 + 8, 64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.activ1 = nn.ReLU()\n",
    "        self.linear_layer_p = nn.Linear(64, 32)\n",
    "        self.dropout_p = nn.Dropout(0.5)\n",
    "        self.activ_p = nn.ReLU()\n",
    "        self.linear_layer2 = nn.Linear(32, 2)\n",
    "        self.activ2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, images, features):\n",
    "        cnn2 = self.cnn_layer2(self.cnn_layer1(images.unsqueeze(1)))\n",
    "        cnn_vec = cnn2.reshape(cnn2.shape[0], -1)\n",
    "        out = self.dropout1(self.activ1(self.linear_layer1(torch.cat((cnn_vec, features), dim=1))))\n",
    "        return self.activ2(self.linear_layer2(self.activ_p(self.dropout_p(self.linear_layer_p(out)))))\n",
    "    \n",
    "clf = load('../models/rule_based.joblib') # The classifier\n",
    "scaler = load('../models/scaler.joblib') # The scaler, transforms formants so that they have a mean of 0 and a variance of 1\n",
    "regressor = torch.load('../models/neural_regressor.pt') # The vowel detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2f7e9",
   "metadata": {},
   "source": [
    "## Run once per file (config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "883d810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'examples/max-noise.wav' # Put your file here\n",
    "target_vowel = '9' # Possible values: 'a', 'e', 'E', 'i', 'o', 'O', 'u', 'y', '2', '9'\n",
    "speaker_gender = 'f' # Possible values: 'f' or 'm'\n",
    "previous_phoneme = 's' # Possible values: 'l', 'm', 'p', 's', 't' or 't1' (last one shouldn't be used)\n",
    "word_ends_with_r = True # True if there is an /R/ after the vowel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59385651",
   "metadata": {},
   "source": [
    "## Run once per file (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf01e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0925625\n",
      "Vowel  Confidence\n",
      "-------------------------\n",
      "2      0.056 =====\n",
      "9      0.270 ===========================\n",
      "a      0.102 ==========\n",
      "e      0.020 ==\n",
      "E      0.086 ========\n",
      "i      0.024 ==\n",
      "O      0.252 =========================\n",
      "o      0.086 ========\n",
      "u      0.026 ==\n",
      "y      0.078 =======\n",
      "Prediction: /9/, confidence: 0.270\n"
     ]
    }
   ],
   "source": [
    "idx2key = ['2', '9', 'a', 'a~', 'e', 'E', 'i', 'O', 'o', 'o~', 'u', 'U~+', 'y'] # All possible vowels\n",
    "valid = [0, 1, 2, 4, 5, 6, 7, 8, 10, 12] # Vowels we consider here (depends on the classifier)\n",
    "all_phonemes = ['l', 'm', 'p', 's', 't', 't1'] # Phonemes that can be before the vowel\n",
    "\n",
    "tmp_wav = 'tmp_process.wav'\n",
    "tmp_wav_2 = 'tmp_process_trimmed.wav'\n",
    "max_w = 31 # Image width to resize to\n",
    "\n",
    "# Remove leading and trailing silences\n",
    "sound = AudioSegment.from_file(input_file)\n",
    "trim_leading_silence = lambda x: x[detect_leading_silence(x, silence_threshold=-25):]\n",
    "trimmed = trim_leading_silence(trim_leading_silence(sound).reverse()).reverse()\n",
    "trimmed.export(tmp_wav, format='wav', bitrate='768k')\n",
    "    \n",
    "# Generate log-melspectrogram\n",
    "y, sr = librosa.load(tmp_wav)\n",
    "mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=512, hop_length=512)\n",
    "mels = np.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "\n",
    "# Rescale mel-spectrogram\n",
    "mels_std = (mels - mels.min()) / (mels.max() - mels.min())\n",
    "melspec = (mels_std * 255).astype(np.uint8)\n",
    "\n",
    "melspec = np.flip(melspec, axis=0) # put low frequencies at the bottom in image\n",
    "melspec = 255-melspec\n",
    "\n",
    "# Feed log-melspectrogram to regression model to predict start and and of the vowel\n",
    "melspec = resize(melspec, (melspec.shape[0], max_w), anti_aliasing=False)\n",
    "input_tensor = torch.tensor(melspec).float().cuda()\n",
    "input_features = torch.tensor([speaker_gender == 'f', not word_ends_with_r, *[previous_phoneme == x for x in all_phonemes]]).cuda()\n",
    "pred = regressor(input_tensor.unsqueeze(0), input_features.unsqueeze(0))[0]\n",
    "vowel_start = pred[0].item()\n",
    "vowel_end = pred[1].item()\n",
    "if vowel_start >= vowel_end:\n",
    "    print('The model predicted that the vowel has negative duration!')\n",
    "    raise ValueError()\n",
    "\n",
    "# Trim file at start and end to only have the vowel\n",
    "sample_rate, wave_data = wavfile.read(tmp_wav)\n",
    "duration = len(wave_data) / sample_rate\n",
    "start_sample = int(duration * vowel_start * sample_rate)\n",
    "end_sample = int(duration * vowel_end * sample_rate)\n",
    "wavfile.write(tmp_wav_2, sample_rate, wave_data[start_sample:end_sample])\n",
    "duration = len(wave_data[start_sample:end_sample]) / sample_rate\n",
    "print(duration)\n",
    "\n",
    "# Extract formants\n",
    "sound = parselmouth.Sound(tmp_wav_2)\n",
    "pointProcess = praat.call(sound, \"To PointProcess (periodic, cc)\", math.ceil(3/duration + 0.000001), 300)\n",
    "formants = praat.call(sound, \"To Formant (burg)\", 0, 5, 5000, 0.025, 50)\n",
    "numPoints = praat.call(pointProcess, \"Get number of points\")\n",
    "f_lists = [[] for i in range(5)]\n",
    "for point in range(1, numPoints + 1):\n",
    "    t = praat.call(pointProcess, \"Get time from index\", point)\n",
    "    for i in range(4):\n",
    "        f_lists[i].append(praat.call(formants, \"Get value at time\", i+1, t, 'Hertz', 'Linear'))\n",
    "f_lists = [[x for x in f_list if not math.isnan(x)] for f_list in f_lists]\n",
    "# Compute the average of formants\n",
    "formants = []\n",
    "try:\n",
    "    for i in range(4):\n",
    "        formants.append(sum(f_lists[i]) / len(f_lists[i]))\n",
    "except ZeroDivisionError:\n",
    "    print('The file is too short to analyze!')\n",
    "    raise\n",
    "\n",
    "# Add additional features (gender, previous phoneme)\n",
    "input_features = torch.cat([input_features[0:1], input_features[2:]]).cpu()\n",
    "features = torch.cat((torch.tensor(formants), input_features)).numpy()\n",
    "\n",
    "# Rescale formants\n",
    "features[:4] = scaler.transform(np.array(features[:4]).reshape(1, -1))[0]\n",
    "\n",
    "# Prediction with probabilities\n",
    "pred = clf.predict_proba([features]) # Probabilities\n",
    "final_vowel = np.argmax(pred)\n",
    "final_confidence = pred[0][final_vowel] # Best score\n",
    "final_vowel = idx2key[valid[final_vowel]] # Actual prediction\n",
    "\n",
    "print('Vowel ', 'Confidence')\n",
    "print('-'*25)\n",
    "for i in range(len(valid)):\n",
    "    vowel = idx2key[valid[i]]\n",
    "    print(f'{vowel:<6} {pred[0][i]:.3f}', '='*int(pred[0][i]*100))\n",
    "\n",
    "print(f'Prediction: /{final_vowel}/, confidence: {final_confidence:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13bb577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lambda Notebook (Python 3)",
   "language": "python",
   "name": "lambda-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
